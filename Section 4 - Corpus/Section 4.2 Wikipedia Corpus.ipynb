{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3ad106",
   "metadata": {},
   "source": [
    "# Section 4.2 Wikipedia Corpus\n",
    "In this section, we are going to build up a corpus based on the wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c0fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959cf3e",
   "metadata": {},
   "source": [
    "## 4.2.1 Wikipeida Corpus Extractor\n",
    "The wikipedia corpus is provided by the wikipedia including a huge number of articles (https://dumps.wikimedia.org/backup-index.html). The dataset we downloaded is in a .xml format so we need to use the codes below to transform them into a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142fa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    " \n",
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48e1652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 04:30:51,767: INFO: running D:\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\byc33\\AppData\\Roaming\\jupyter\\runtime\\kernel-ccff2282-94a8-46a2-bea6-174b4b863dc7.json\n",
      "2023-11-26 04:32:12,211: INFO: Saved 10000 articles.\n",
      "2023-11-26 04:33:26,587: INFO: Saved 20000 articles.\n",
      "2023-11-26 04:34:25,852: INFO: Saved 30000 articles.\n",
      "2023-11-26 04:35:30,289: INFO: Saved 40000 articles.\n",
      "2023-11-26 04:36:20,947: INFO: Saved 50000 articles.\n",
      "2023-11-26 04:36:44,784: INFO: Saved 60000 articles.\n",
      "2023-11-26 04:37:06,662: INFO: Saved 70000 articles.\n",
      "2023-11-26 04:37:26,937: INFO: Saved 80000 articles.\n",
      "2023-11-26 04:38:11,625: INFO: Saved 90000 articles.\n",
      "2023-11-26 04:39:02,922: INFO: Saved 100000 articles.\n",
      "2023-11-26 04:39:02,958: INFO: Finished Saved 100000 articles.\n"
     ]
    }
   ],
   "source": [
    "program = os.path.basename( sys.argv[0] )\n",
    "logger = logging.getLogger(program)\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logger.info( \"running %s\" % ' '.join(sys.argv) )\n",
    " \n",
    "inp = 'D:/EN-Wiki/enwiki-latest-pages-articles.xml.bz2'\n",
    "outp = 'D:/EN-Wiki/wiki.txt'\n",
    "space = ' '\n",
    "i = 0\n",
    "output = open(outp, 'w', encoding='utf-8')\n",
    "#gensim里的维基百科处理类WikiCorpu\n",
    "wiki = WikiCorpus(inp, dictionary=[] )\n",
    " \n",
    "#通过get_texts将维基里的每篇文章转换为1行text文本，并且去掉了标点符号等内容\n",
    "for text in wiki.get_texts():\n",
    "    output.write( space.join(text) + '\\n' )\n",
    "    i += 1\n",
    "    if ( i % 10000 == 0):\n",
    "        logger.info('Saved ' + str(i) + ' articles.')\n",
    "    if i == 100000:\n",
    "        break\n",
    "output.close()\n",
    "logger.info('Finished Saved ' + str(i) + ' articles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe2312",
   "metadata": {},
   "source": [
    "## 4.2.2 Split into small .txt file\n",
    "We have extract a part of the whole corpus which are large enough for our further usage. However, the .txt file we acquire are too big to process directely, so we split it into small pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964cc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_text(input_file, output_prefix, chunk_size):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_file = f\"{output_prefix}_part_{i + 1}.txt\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc11339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"D:/EN-Wiki/wiki.txt\"  # 大文本文件的路径\n",
    "output_prefix = \"D:/EN-Wiki/wiki-seg\"  # 输出文件的前缀\n",
    "chunk_size = 10000000  # 指定每个小文件的大小，可以根据需要调整\n",
    "\n",
    "split_large_text(input_file, output_prefix, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bbdef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
